{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluating Predictive Accuracy of a Binary Classifier (Python)\n",
    "\n",
    "def evaluate_classifier(predicted, observed):\n",
    "    import pandas as pd \n",
    "    if(len(predicted) != len(observed)):\n",
    "        print('\\nevaluate_classifier error:',\\\n",
    "             ' predicted and observed must be the same length\\n')\n",
    "        return(None) \n",
    "    if(len(set(predicted)) != 2):\n",
    "        print('\\nevaluate_classifier error:',\\\n",
    "              ' predicted must be binary\\n')\n",
    "        return(None)          \n",
    "    if(len(set(observed)) != 2):\n",
    "        print('\\nevaluate_classifier error:',\\\n",
    "              ' observed must be binary\\n')\n",
    "        return(None)          \n",
    "\n",
    "    predicted_data = predicted\n",
    "    observed_data = observed\n",
    "    input_data = {'predicted': predicted_data,'observed':observed_data}\n",
    "    input_data_frame = pd.DataFrame(input_data)\n",
    "    \n",
    "    cmat = pd.crosstab(input_data_frame['predicted'],\\\n",
    "        input_data_frame['observed']) \n",
    "    a = float(cmat.ix[0,0])\n",
    "    b = float(cmat.ix[0,1])\n",
    "    c = float(cmat.ix[1,0]) \n",
    "    d = float(cmat.ix[1,1])\n",
    "    n = a + b + c + d\n",
    "    predictive_accuracy = (a + d)/n #accuracy \n",
    "\n",
    "# a=TP True Positives\n",
    "# b=FP False Positive\n",
    "# c=FN False Negatives\n",
    "# d=TN True Negative   \n",
    "    \n",
    "#--------------True positive Rate / Sensitivity / Recall / Hit Rate-----------------\n",
    "#is the proportion of positive cases that were correctly identified\n",
    "#  TP / (TP + FN)\n",
    "\n",
    "    true_positive_rate = a / (a + c)\n",
    "    sensitivity = a / (a + c)\n",
    "    recall = a / (a + c)\n",
    "    hit_rate = a / (a + c)\n",
    "\n",
    "#-------------True Negative Rate / Specificity -----------------------\n",
    "# TN / (FP + TN)\n",
    "    \n",
    "    true_negative_rate = d / (b + d)\n",
    "    specificity = d / (b + d)\n",
    "\n",
    "#----------Precision or Positive Predictive Value---------------------\n",
    "#is the proportion of the predicted positive cases that were correct \n",
    "# TP / (TP + FP)\n",
    "    precision = a / (a + b)\n",
    "    positive_predictive_value = a / (a + b)\n",
    "\n",
    "#---------------False Positive Rate or Fall Out---------------------------    \n",
    "#false positive rate (FP) is the proportion of negatives cases that were \n",
    "#incorrectly classified as positive\n",
    "# FPR = FP / (FP + TN)\n",
    "# FPR = 1 - specificity\n",
    "#FPR = 1 - True Negative Rate\n",
    "    false_positive_rate = b / (b + d)\n",
    "    fall_out = b / (b + d)\n",
    "    false_positive_rate2 = 1 - specificity\n",
    "    fall_out2 = 1 - specificity    \n",
    "    \n",
    "#---------------False Negative Rate or Miss Rate --------------------------\n",
    "#is the proportion of positives cases that were incorrectly classified as negative\n",
    "# a=TP True Positives\n",
    "# b=FP False Positive\n",
    "# c=FN False Negatives\n",
    "# d=TN True Negative  \n",
    "#FN / (FN + TP)\n",
    "    false_negative_rate = c / (c + a)\n",
    "    miss_rate = c / (c + a)\n",
    "    false_negative_rate2 = 1 - sensitivity\n",
    "    miss_rate2 = 1 - sensitivity\n",
    "\n",
    "#------------------other------------------\n",
    "    sensitivity2 = 1 - false_negative_rate\n",
    "    specificity2 = 1 - false_positive_rate\n",
    "\n",
    "#-----------negative Predictive value------------------    \n",
    "# TN / (TN + FN)\n",
    "    negative_predictive_value = d / (d + c)\n",
    "    \n",
    "#-----------false discovery rate ---------------------\n",
    "# FP / (TP + FP)\n",
    "# 1 - precision\n",
    "    false_discovery_rate = b / (a + b)\n",
    "    false_discovery_rate2 = 1 - precision\n",
    "\n",
    "#----------------informedness------------------------\n",
    "#sensitivity + specificity - 1\n",
    "# true positive rate + true negative rate - 1\n",
    "    informedness = sensitivity + specificity - 1\n",
    "    informedness2 = true_positive_rate + true_negative_rate - 1\n",
    "\n",
    "#------------------markedness----------------------------\n",
    "#precision + negative predictive value\n",
    "#positive predictive value + negative predictive value\n",
    "    markedness = precision + negative_predictive_value\n",
    "\n",
    "    #----------expected accuracy----------------------\n",
    "    expected_accuracy = (((a + b)*(a + c)) + ((b + d)*(c + d)))/(n * n)\n",
    "\n",
    "    #------------kappa---------------------------------\n",
    "    kappa = (predictive_accuracy - expected_accuracy)/(1 - expected_accuracy)   \n",
    "\n",
    "    return(a, b, c, d, predictive_accuracy,\n",
    "           true_positive_rate, sensitivity, recall,hit_rate, sensitivity2,\n",
    "           true_negative_rate, specificity, precision, positive_predictive_value, specificity2,\n",
    "           false_positive_rate, fall_out, false_positive_rate2, fall_out2,\n",
    "           false_negative_rate, miss_rate, false_negative_rate2, miss_rate2,\n",
    "           negative_predictive_value, \n",
    "           false_discovery_rate, false_discovery_rate2, \n",
    "           informedness, informedness2, \n",
    "           markedness,\n",
    "           expected_accuracy,\n",
    "           kappa)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
